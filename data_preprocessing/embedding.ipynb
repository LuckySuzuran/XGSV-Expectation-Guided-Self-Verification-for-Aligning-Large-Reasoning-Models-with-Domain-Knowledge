{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd47a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"xxx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\n",
    "model = AutoModel.from_pretrained(model_path).to(device)\n",
    "for param in model.parameters(): #froze all parameters of LLM\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb086e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "    \n",
    "def get_embedding(input_texts, max_length):\n",
    "    max_length = max_length\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(\n",
    "        input_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_dict.to(model.device)\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    return embeddings.detach().cpu(), outputs.last_hidden_state.detach().cpu(), batch_dict['attention_mask'].detach().cpu()\n",
    "\n",
    "def get_embedding_batch(input_texts: list, \n",
    "                       batch_size: int = 8, \n",
    "                       max_length: int = 8192) -> np.ndarray:\n",
    "    embeddings = []\n",
    "    last_hidden_states = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    progress_bar = tqdm(total=len(input_texts)//batch_size + 1, \n",
    "                       desc=\"Generating embeddings\",\n",
    "                       unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_texts), batch_size):\n",
    "            batch_texts = input_texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            batch_dict = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_emb = last_token_pool(outputs.last_hidden_state, \n",
    "                                      batch_dict['attention_mask'])\n",
    "            \n",
    "            embeddings.append(batch_emb.detach().cpu())\n",
    "            last_hidden_states.append(outputs.last_hidden_state.detach().cpu())\n",
    "            attention_masks.append(batch_dict['attention_mask'].detach().cpu())\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    return torch.concatenate(embeddings, axis=0), torch.concatenate(last_hidden_states,axis=0), torch.concatenate(attention_masks, axis=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
